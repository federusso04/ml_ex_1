{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Riduzione della Dimensionalità**","metadata":{}},{"cell_type":"markdown","source":"## Dataset MNIST_784: Panoramica\n\nIl dataset **MNIST_784** è un dataset molto popolare di cifre scritte a mano, ampiamente utilizzato in machine learning e deep learning per **classificazione di immagini**.\n\n## Caratteristiche Chiave:\n**Tipo di dati**: Immagini\n\n* **Contenuto**: Cifre scritte a mano (da 0 a 9)\n\n* **Numero di Campioni**: 70.000 immagini\n\n* **Dimensione Immagine**: 28x28 pixel (appiattite in 784 caratteristiche per campione) \n\n* **Caratteristiche delle Immagini**: Ogni immagine è rappresentata come un array 1D di 784 valori di intensità dei pixel (scala di grigi)\n\n* **Classi**: 10 classi (cifre da 0 a 9).\n\nIl dataset può essere accessibile direttamente attraverso piattaforme come `scikit-learn`\n\n## Perché \"784\"?\n\nIl nome \"mnist_784\" fa riferimento ai 784 pixel che compongono l' immagine. Supponendo infatti di calcolare quanti pixel contiene la singola immagine troviamo che:\n\n**28 x 28 = 784**\n","metadata":{}},{"cell_type":"markdown","source":"Digitalmente un' immagine è una **matrice** contenente valori numerici discreti nell' intervallo [0, 255] che rappresentano **l' intensità del pixel** in scala di grigi. \n\nPer quanto riguarda le immagini a colori viene introdotta una terza dimensione che è il **canale**. Tipicamente un' immagine a colore è in formato **RGB (Red, Green, Blue)** e possiede 3 canali. \n\nUn pixel sarà quindi la combinazione del valore di Rosso, Verde e Blu per quel singolo pixel.","metadata":{}},{"cell_type":"markdown","source":"Nel caso del dataset **mnist_784** ogni singola immagine viene considerata come vettore unidimensionale (1D), in cui tutti i pixel vengono allineati.\n\nPer semplicità consideriamo un' immagine di risoluzione 3x3:\n\n```\n [255, 128, 64]\n [ 0, 128, 192]\n [ 64, 255,  0]\n```\n\nSe trasformo l' immagine in un vettore unidimensionale ottengo:\n\n```\n[255, 128, 64, 0, 128, 192, 64, 255, 0]\n```","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import fetch_openml\n\n# 1: Carichiamo il dataset MNIST\nmnist = fetch_openml('mnist_784', version=1, as_frame=False,parser='auto')\nX = mnist.data\ny = mnist.target.astype(int)","metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Funzione per stampare alcune immagini\ndef display_images(images, labels, num_images=5):\n    fig, axes = plt.subplots(1, num_images, figsize=(10, 3))\n    for i in range(num_images): \n        axes[i].imshow(images[i].reshape(28, 28), cmap='gray') \n        axes[i].set_title(f'Label: {labels[i]}')\n        axes[i].axis('off')\n    plt.show()\n\n# Decidiamo il numero di immagini da visualizzare e stampiamole utilizzando la funzione display_images\nnum_samples = 5\ndisplay_images(X[:num_samples], y[:num_samples], num_images=num_samples)\n","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x300 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"execution_count":39},{"cell_type":"markdown","source":"### **Metodo `.shape`**","metadata":{}},{"cell_type":"markdown","source":"Un metodo molto utile degli array, soprattutto nel caso di array molto grandi, è:\n\n- [numpy.shape](https://numpy.org/doc/2.1/reference/generated/numpy.shape.html): Stampa le dimensioni di un array.","metadata":{}},{"cell_type":"code","source":"a = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\na.shape","metadata":{},"outputs":[{"data":{"text/plain":["(2, 5)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"execution_count":40},{"cell_type":"markdown","source":"## **Principal Component Analysis (PCA)**\n\nLa **PCA** è una tecnica utilizzata per ridurre la dimensionalità dei dati preservando la maggior parte della variazione presente nel dataset originale. \n\nL'obiettivo della PCA è trasformare i dati in un nuovo sistema di coordinate, in cui le prime componenti principali catturano la maggior parte della varianza. Questo può semplificare l'analisi e la visualizzazione dei dati, riducendo il rumore e migliorando l'efficienza computazionale.\n\n### **Come si usa la PCA?**\n\nPer applicare la PCA ci sono degli step da seguire:\n\n1. **Standardizzazione dei dati**: Per rendere le features comparabili tra loro viene eseguita la standardizzazione.\n\n2. **Calcolo della matrice della covarianza**: rappresente la variazione di ogni variabile rispetto alle altre (inclusa se stessa).\n\n3. **Calcolo degli autovalori e autovettori**: Vengono calcolati gli autovalori della matrice di covarianza, che rappresentano la varianza espressa dalle componenti principali, e gli autovalori, che rappresentano le direzioni delle componenti principali. \n\n4. **Ordinamento**: Si ordnano gli autovalori (e i corrispettivi autovettori) in ordine decresente in base alla varianza espressa.\n\n5. **Selezione delle top-n componenti**: Si scelgono il numero di componenti desiderato che più esprimono varianza.\n\n6. **Trasformazione dei dati**: SI proiettano i dati lungo le componenti principali scelte, ottenendo la riduzione di dimensione desiderata.","metadata":{}},{"cell_type":"markdown","source":"## **Step 1 - Standardizzazione**\n\nPer standardizzare i dati applichiamo la **Z-normalization** che consiste nel trasformare i dati in modo che abbiano:\n\n    - media = 0\n    - deviazione standard = 1\n\nPer fare ciò utilizzeremo le funzioni di numpy:\n\n- [numpy.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html): Calcola la media lungo un asse specifico.\n- [numpy.std](https://numpy.org/doc/stable/reference/generated/numpy.std.html): Calcola la deviazione standard lungo un asse specifico.","metadata":{}},{"cell_type":"markdown","source":"### **Broadcasting**\n\nPrima di procedere però osserviamo una proprietà fondamentale che offre NumPy. ","metadata":{}},{"cell_type":"code","source":"prezzi = np.array([100, 200, 300, 400])  # Array NumPy\nsconto = 0.10  # Valore scalare\n\nprezzi_scontati = prezzi - prezzi * sconto  # Broadcasting automatico\n\nprint(prezzi_scontati)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 90. 180. 270. 360.]\n"]}],"execution_count":41},{"cell_type":"code","source":"voti = np.array([[80, 85, 90], \n                 [75, 80, 95], \n                 [60, 70, 80]])  # 3 studenti con 3 voti ciascuno\n\nbonus = np.array([5, 10, 15])  # Bonus per ogni materia\n\nvoti_finali = voti + bonus  # Broadcasting applicato!\n\nprint(voti_finali)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 85  95 105]\n"," [ 80  90 110]\n"," [ 65  80  95]]\n"]}],"execution_count":42},{"cell_type":"markdown","source":"Notiamo inoltre come in alcune funzioni di NumPy è possibile specificare l' **asse** lungo cui applicarle. Specificando l' asse tra gli argomenti della funzione possiamo ottenere diversi risultati.","metadata":{}},{"cell_type":"code","source":"A = np.array([[1, 2, 3], \n              [4, 5, 6], \n              [7, 8, 9]])\n\n# Calcoliamo la media di A\n\nmean_A = np.mean(A, axis=0) # axis = 0 indica che vogliamo calcolare la media sulle colonne\n                            # axis = 1 indica che vogliamo calcolare la media sulle righe\n                            # axis = None indica che vogliamo calcolare la media su tutti gli elementi\n\nprint(f'Media di A: {mean_A}')","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Media di A: [4. 5. 6.]\n"]}],"execution_count":43},{"cell_type":"code","source":"# Calcoliamo la media dei dati\n# svolgimento...\nmedia = np.mean(X, axis=0)\n\n# Sottraggo la media da ogni riga di X\n# svolgimento...\ndati_centrati = X - media\n\n# Calcoliamo la deviazione standard dei dati centrati\n# svolgimento...\ndev_std = np.std(dati_centrati, axis=0)\n\n# Per standardizzare i dati utilizzo la formula: dati_centrati / deviazione_standard * 1e-6\n# 1e-6 è un valore piccolo aggiunto per evitare divisioni per zero\n# svolgimento...\ndati_standardizzati = dati_centrati / (dev_std + 1e-6)\n","metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## **Step 2 - Calcolo della matrice di covarianza**","metadata":{}},{"cell_type":"markdown","source":"Per calcolare la matrice di covarianza utilizziamo la funzione di NumPy:\n\n- [numpy.cov](https://numpy.org/doc/stable/reference/generated/numpy.cov.html): Calcola la matrice di covarianza dei dati.","metadata":{}},{"cell_type":"code","source":"# Calcoliamo la matrice di covarianza utilizzando la funzione np.cov a cui dobbiamo passare come parametro i dati standardizzati.\n\n# N.B. la funzione np.cov richiede di default tratta le righe come variabili e le colonne come osservazioni\n# nel nostro caso vogliamo fare il contrario, quindi dobbiamo specificare il parametro rowvar=False\n\n# svolgimento...\ncov_matrix = np.cov(dati_standardizzati, rowvar=False)","metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## **Step 3 - Calcolo degli autovalori e autovettori**","metadata":{}},{"cell_type":"markdown","source":"Per calcolare autovalori e autovettori utilizziamo la funzione di NumPy:\n\n- [numpy.linalg.eigh](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html): Calcola autovalori e autovettori di una matrice simmetrica.","metadata":{}},{"cell_type":"code","source":"# Calcoliamo gli autovalori e autovettori della matrice di covarianza. Utilizziamo la funzione np.linalg.eigh a cui passiamo come parametro la matrice di covarianza.\n\n# N.B. la funzione np.linalg.eigh restituisce una tupla con due elementi, rispettivamente:\n# - un array con gli autovalori in ordine crescente\n# - una matrice con gli autovettori corrispondenti\n# a noi interessa salvarli entrambi in due variabili distinte\n\n\n# svolgimento...\nautovalori, autovettori = np.linalg.eigh(cov_matrix)","metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"## **Step 4 - Ordinare gli autovalori e autovettori**","metadata":{}},{"cell_type":"markdown","source":"Per ordinare gli autovalori e autovettori in ordine decrescente utilizziamo la funzione di NumPy:\n- [numpy.argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html): Ritorna gli indici degli autovalori ordinati in ordine crescente.\n\nPoichè a noi interessa l' ordine decrescente, dobbiamo invertire questi indici. Vediamo di seguito un modo veloce e compatto per farlo.","metadata":{}},{"cell_type":"code","source":"lista = [1, 2, 3, 4, 5]\n\n# La notazione di slicing [::-1] segue il ormato [start:stop:step]:\n# - start: indice di partenza\n# - stop: indice di fine\n# - step: passo\n# Se non specifico start e stop, vengono considerati tutti gli elementi\n# Se specifico step = -1, vengono considerati gli elementi in ordine inverso\n\nlista_invertita = lista[::-1]\n\nprint(\"Lista originale:\", lista)\nprint(\"Lista invertita:\", lista_invertita)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Lista originale: [1, 2, 3, 4, 5]\n","Lista invertita: [5, 4, 3, 2, 1]\n"]}],"execution_count":50},{"cell_type":"code","source":"# Sulla base dell' esempio fatto sopra e con la funzione np.argsort ordiniamo in ordine decrescente gli autovalori.\n# Lo stesso ordinamento deve essere poi eseguito anche sugli autovettori.\n\n# Estraiamo gli indici degli autovalori ordinari in ordine decrescente\n\n# svolgimento...\nsorted_idx = np.argsort(autovalori)[::-1]\n\n\n# Ordiniamo gli autovalori utilizzando i nuovi indici\n\n# svolgimento...\nautovalori_ordinati = autovalori[sorted_idx]\n\n# Ordiniamo gli autovettori utilizzando i nuovi indici\n# N.B. Nella matrice degli autovalori ogni colonna rappresenta un autovettore, quindi dobbiamo ordinare le colonne e non le righe.\n\n# svolgimento...\nautovettori_ordinati = autovettori[:, sorted_idx]\n\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(784,)\n"]}],"execution_count":51},{"cell_type":"markdown","source":"## **Step 5 - Selezionare le top-n componenti**","metadata":{}},{"cell_type":"markdown","source":"Selezioniamo le top-n componenti che esprimono maggiore varianza nel dataset. In generale, per avere una migliore rappresentazione grafica del dataset si scelgono le migliori 2 componenti.","metadata":{}},{"cell_type":"code","source":"# Selezioniamo il numero di componenti che vogliamo mantenere\nn_components = 2\n\n# Selezionare gli autovettori corrispondenti \n# ATTENZIONE: valutiamo correttamente come fare lo slicing. Dobbiamo utilizzare gli indici selezionati \n# per selezionare gli autovettori corrispondenti\n\n# svolgimento...\ncomponenti_principali = autovettori_ordinati[:, :n_components]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Calcoliamo la varianza espressa**","metadata":{}},{"cell_type":"markdown","source":"Calcoliamo quanta varianza viene espressa dal numero di componenti che abbiamo scelto.","metadata":{}},{"cell_type":"code","source":"# Per calcoalre la percentuale di varianza espressa diviamo gli autovalori selezionati \n# per la somma di tutti gli autovalori \n# Per calcolare la somma di tutti gli autovalori potrebbe essere utile utilizzare la funzione sum()\n\n# svolgimento...\nvarianza_spiegata = autovalori_ordinati[:n_components] / np.sum(autovalori_ordinati)\n","metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"## **Step 6 - Transformare i dati**","metadata":{}},{"cell_type":"markdown","source":"Riduciamo la dimensione del dataset proiettando i dati lungo le componenti principali che abbiamo calcolato. Per ottenere questo risultato utilizziamo la funzione di NumPy:\n\n- [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html): Calcola il prodotto riga colonna di due array.","metadata":{}},{"cell_type":"code","source":"# Proiettiamo i dati nello spazio delle componenti principali utilizzando np.dot a cui dobbiamo \n# passare come parametri i dati, e le direzioni lungo cui proiettare.\n# N.B. Dobbiamo proiettare i dati standardizzati\n\n# svolgimento...\ndati_proiettati = np.dot(dati_standardizzati, componenti_principali)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Visualizzazione dei dati** ","metadata":{}},{"cell_type":"markdown","source":"Utilizzando la libreria `matplotlib` per visualizzare i dati trasformati","metadata":{}},{"cell_type":"code","source":"def plot_pca_2d(X_transformed, y, explained_variance):\n    \"\"\"\n    Funzione per visualizzare i dati trasformati in 2D con PCA.\n\n    Parametri:\n    - X_transformed: array numpy con le prime due componenti principali (n_samples, 2)\n    - y: array numpy contenente le etichette delle classi\n    - explained_variance: float, percentuale di varianza spiegata dalle due componenti principali\n    \n    Output:\n    - Un grafico scatter delle immagini nello spazio PCA 2D.\n    \"\"\"\n    plt.figure(figsize=(10, 7))\n    scatter = plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap='viridis', alpha=0.5)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.title(f\"2D plot of Images using PCA with {explained_variance*100:.2f}% variance\")\n    plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chiamare la funzione plot_pca_2d per visualizzare i dati trasformati in 2D\n\n# svolgimento...\nplot_pca_2d(dati_proiettati, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_pca_with_images(X, X_transformed, y):\n    \"\"\"\n    Funzione per visualizzare le immagini proiettate nello spazio delle prime due componenti principali della PCA.\n\n    Parametri:\n    - X: array numpy (n_samples, n_features), dataset originale con le immagini appiattite\n    - X_transformed: array numpy (n_samples, 2), dataset proiettato sulle prime due componenti principali della PCA\n    - y: array numpy contenente le etichette delle classi\n    - image_shape: tuple, dimensione originale delle immagini (default: (28, 28) per MNIST)\n    - N: int, numero massimo di immagini da visualizzare per classe (default: 20)\n    - save_path: str, se fornito, salva il grafico in un file con il percorso specificato\n    \n    Output:\n    - Un grafico con le immagini sovrapposte alla rappresentazione PCA.\n    \"\"\"\n\n    N = 20\n    image_shape=(28, 28)\n    plt.figure(figsize=(8, 6))\n    \n    num_classes = len(np.unique(y))\n    image_size = 1.5  # Dimensione delle immagini nel plot\n    \n    for i in range(num_classes):\n        indices = y == i\n        images = X[indices]\n        points = X_transformed[indices]\n        \n        images = images[:N]\n        points = points[:N]\n        \n        for image, point in zip(images, points):\n            image = image.reshape(image_shape)\n            plt.imshow(image, extent=(point[0] - image_size / 2, point[0] + image_size / 2,\n                                      point[1] - image_size / 2, point[1] + image_size / 2),\n                       cmap='gray', aspect='auto')\n    \n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.xlim(-10, 27)\n    plt.ylim(-17, 15)\n    \n    plt.show()\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chiamare la funzione plot_pca_with_images per visualizzare le immagini proiettate nello spazio delle prime due componenti principali della PCA\n\n\n# svolgimento...\nplot_pca_with_images(dati_proiettati, X, y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **PCA con scikit-learn**\n\nPossiamo applicare la PCA anche utilizzando il pacchetto `scikit-learn`. Con quest' ultimo infatti l' intero processo visto precedentemente è parecchio semplificato. \n\nGli step da seguire in questo caso sono:\n\n1. **Caricare e standardizzare i dati**\n\n2. **Applicare PCA**\n\n3. **Calcolare la varianza espressa**\n\n4. **Visualizzare i dati nel nuovo spazio**","metadata":{}},{"cell_type":"markdown","source":"## **Step 1 - Caricare e standardizzare i dati**\n\nPer caricare i dati la procedura è uguale a quanto fatto in precedenza. Per quanto riguardala standardizzazione invece utilizzeremo direttamente l'oggetto di `scikit-learn`:\n\n- [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html): Standardizza i dati rimuovendo la media e scalandoli a varianza unitaria. ","metadata":{}},{"cell_type":"code","source":"# Carichiamo il dataset.\nmnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\nX = mnist.data       \ny = mnist.target.astype(int)  \n\n# Inizializziamo un' istanza di `StandardScaler`, non è necessario specificare alcun parametro.\n# svolgimento...\nscaler = StandardScaler()\n\n# Utilizziamo lo scaler sui nostri dati. \n# Lo scaler possiede il medooto .fit_transform che utilizzeremo per scalare i dati. Questo metodo ha bisogno dei dati come parametro.\n\n# svolgimento...\nX_scaled = scaler.fit_transform(X)","metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"## **Step 2 - Applicare la PCA**","metadata":{}},{"cell_type":"markdown","source":"Per applicare la PCA utilizziamo direttamente l' implementazione di `scikit-learn`:\n\n- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html): Implementazione di Principal Component Analysis (PCA) in `scikit-learn`.\n\n","metadata":{}},{"cell_type":"code","source":"# Istanziamo la classe PCA specificando come parametro il numero di componenti.\n# Utilizziamo il metodo fit_transform della PCA per calcolarle.\n\nn_components = 2\n\n# svolgimento...\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X_scaled)","metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"## **Step 3 - Calcolare la varianza espressa**","metadata":{}},{"cell_type":"markdown","source":"L' oggetto PCA appena istanziato possiede un attributo che ci permette di valutare quanta varianza viene espressa:\n\n- [PCA - explained_variance_ratio_](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) L' attributo `explained_variance_ratio_` nella PCA di `scikit-learn` ci da la varianza espressa dalla PCA.","metadata":{}},{"cell_type":"code","source":"# Estrarre e stampare la varianza espressa tramite l' attributo explained_variance_ratio_ della PCA.\n\n# svolgimento...\nprint(\"Varianza espressa dalle componenti PCA:\", pca.explained_variance_ratio_)","metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"## **Step 4 - Visualizzare i dati nel nuovo spazio**","metadata":{}},{"cell_type":"code","source":"# Visualizziamo i risultati utilizzando le stesse funzioni viste in precedenza \n\n# svolgimento...\nplot_pca_2d(X_pca, y)\nplot_pca_with_images(X_pca, X, y)","metadata":{},"outputs":[],"execution_count":null}]}